{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_checkpoint(model_size, checkpoint_step, rotate_vector=False):\n",
    "    \"\"\"\n",
    "    Remove the checkpoint vector (checkpoint n+1 - checkpoint n) from the model and return the model.\n",
    "    Rotate vector: instead of removing the checkpoint vector, remove a random vector of the same norm.\n",
    "    \"\"\"\n",
    "    model_checkpoint = GPTNeoXForCausalLM.from_pretrained(\n",
    "        f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "        revision=f\"step{checkpoint_step}\",\n",
    "        cache_dir=f\"./pythia-{model_size}-deduped/\",\n",
    "    )\n",
    "    model_next_checkpoint = GPTNeoXForCausalLM.from_pretrained(\n",
    "        f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "        revision=f\"step{checkpoint_step + 1000}\",\n",
    "        cache_dir=f\"./pythia-{model_size}-deduped/\",\n",
    "    )\n",
    "    model_final = GPTNeoXForCausalLM.from_pretrained(\n",
    "        f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "        cache_dir=f\"./pythia-{model_size}-deduped/\",\n",
    "    )\n",
    "\n",
    "    # remove model_next_checkpoint - model_checkpoint from model_final\n",
    "    for param_final, param_checkpoint, param_next_checkpoint in zip(\n",
    "        model_final.parameters(), model_checkpoint.parameters(), model_next_checkpoint.parameters()\n",
    "    ):\n",
    "        if not rotate_vector:\n",
    "            param_final.data = param_final.data - (param_next_checkpoint.data - param_checkpoint.data)\n",
    "        else:\n",
    "            checkpoint_vector = (param_next_checkpoint.data - param_checkpoint.data)\n",
    "            # random vector of the same norm\n",
    "            random_vector = torch.randn(checkpoint_vector.data.shape)\n",
    "            random_vector = (random_vector / torch.norm(random_vector)) * torch.norm(checkpoint_vector)\n",
    "            param_final.data = param_final.data - random_vector\n",
    "\n",
    "    \n",
    "    return model_final\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same tokenizer for all models, I think it's correct\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    f\"EleutherAI/pythia-70m-deduped\",\n",
    "    cache_dir=f\"./pythia-70m-deduped/\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "n_batch = 100\n",
    "device = \"cuda:2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"wikipedia\", '20220301.en',\n",
    "                       split=\"train\", cache_dir=\".huggingface\")\n",
    "\n",
    "dataset = dataset.select(range(n_batch + 1))\n",
    "dataset = dataset.map(\n",
    "    lambda e: tokenizer(\n",
    "        e[\"text\"], truncation=True, max_length=2048, padding=\"max_length\"\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "dataset = dataset.with_format(type=\"torch\")\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_steps = [20_000, 40_000, 60_000, 80_000, 90_000, 100_000, 110_000, 120_000, 130_000, 140_000]\n",
    "model_sizes = [\"70m\", \"160m\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the models which have been trained without each checkpoint\n",
    "#TODO\n",
    "# models_retrained_dict = dict of dict of models, indexed by model_size and checkpoint_step\n",
    "models_retrained_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {}\n",
    "for model_size in model_sizes:\n",
    "    models_dict[model_size] = {}\n",
    "    for checkpoint_step in checkpoints_steps:\n",
    "        print(f\"Loading model {model_size} step {checkpoint_step}\")\n",
    "        models_dict[model_size][checkpoint_step] = remove_checkpoint(model_size, checkpoint_step, rotate_vector=False)\n",
    "    models_dict[model_size][\"final\"] = GPTNeoXForCausalLM.from_pretrained(\n",
    "        f\"EleutherAI/pythia-{model_size}-deduped\",\n",
    "        cache_dir=f\"./pythia-{model_size}-deduped/\",\n",
    "    )\n",
    "    models_dict[model_size][\"final\"]\n",
    "    models_dict[model_size][\"final\"].to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute the distance between retrained model and model with removed checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We measure the similarity between models by taking the KL divergence on their logit outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_remove = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "losses_retrained = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_remove_vs_retrained = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_retrained_vs_remove = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_remove_vs_final = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_final_vs_remove = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_retrained_vs_final = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "kl_divs_final_vs_retrained = np.zeros((len(model_sizes), len(checkpoints_steps), n_batch))\n",
    "\n",
    "# the code can be sped up if we can load all the models on the GPU at the same time\n",
    "for i, model_size in enumerate(model_sizes):\n",
    "    model_final = models_dict[model_size][\"final\"]\n",
    "    model_final.to(device)\n",
    "    model_final.eval()\n",
    "    for j, checkpoint_step in enumerate(checkpoints_steps):\n",
    "        print(f\"Running model {model_size} step {checkpoint_step}\")\n",
    "        model_remove = models_dict[model_size][checkpoint_step]\n",
    "        model_remove.to(device)\n",
    "        model_remove.eval()\n",
    "        model_retrained = models_retrained_dict[model_size][checkpoint_step]\n",
    "        model_retrained.to(device)\n",
    "        for k, batch in tqdm(enumerate(dataloader)):\n",
    "            if k == n_batch:\n",
    "                break\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs_remove = model_remove(input_ids, labels=input_ids, return_dict=True)\n",
    "                logprobs_remove = torch.nn.functional.log_softmax(outputs_remove.logits, dim=-1)\n",
    "                probs_remove = torch.exp(logprobs_remove)\n",
    "                losses_remove[i, j, k] = outputs_remove.loss.mean().item()\n",
    "                outputs_retrained = model_retrained(input_ids, labels=input_ids, return_dict=True)\n",
    "                logprobs_retrained = torch.nn.functional.log_softmax(outputs_retrained.logits, dim=-1)\n",
    "                probs_retrained = torch.exp(logprobs_retrained)\n",
    "                losses_retrained[i, j, k] = outputs_retrained.loss.mean().item()\n",
    "            outputs_final = model_final(input_ids, return_dict=True)\n",
    "            logprobs_final = torch.nn.functional.log_softmax(outputs_final.logits, dim=-1)\n",
    "            probs_final = torch.exp(logprobs_final)\n",
    "\n",
    "            kl_divs_final_vs_remove[i, j, k] = torch.sum(\n",
    "                probs_final * (logprobs_final - logprobs_remove),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            kl_divs_remove_vs_final[i, j, k] = torch.sum(\n",
    "                probs_remove * (logprobs_remove - logprobs_final),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            kl_divs_retrained_vs_remove[i, j, k] = torch.sum(\n",
    "                probs_retrained * (logprobs_retrained - logprobs_remove),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            kl_divs_remove_vs_retrained[i, j, k] = torch.sum(\n",
    "                probs_remove * (logprobs_remove - logprobs_retrained),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            kl_divs_retrained_vs_final[i, j, k] = torch.sum(\n",
    "                probs_retrained * (logprobs_retrained - logprobs_final),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            kl_divs_final_vs_retrained[i, j, k] = torch.sum(\n",
    "                probs_final * (logprobs_final - logprobs_retrained),\n",
    "                dim=-1,\n",
    "            ).mean().item()\n",
    "            \n",
    "        model_remove.to(\"cpu\")\n",
    "        model_retrained.to(\"cpu\")\n",
    "    model_final.to(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a table of the results with uncertainties, written in scientific notation\n",
    "for i, model_size in enumerate(model_sizes):\n",
    "    for j, checkpoint_step in enumerate(checkpoints_steps):\n",
    "        print(f\"Model {model_size} step {checkpoint_step}\")\n",
    "        print(f\"Loss remove: {losses_remove[i, j].mean():.2e} +- {losses_remove[i, j].std():.2e}\")\n",
    "        print(f\"Loss retrained: {losses_retrained[i, j].mean():.2e} +- {losses_retrained[i, j].std():.2e}\")\n",
    "        print(f\"KL div remove vs retrained: {kl_divs_remove_vs_retrained[i, j].mean():.2e} +- {kl_divs_remove_vs_retrained[i, j].std():.2e}\")\n",
    "        print(f\"KL div retrained vs remove: {kl_divs_retrained_vs_remove[i, j].mean():.2e} +- {kl_divs_retrained_vs_remove[i, j].std():.2e}\")\n",
    "        print(f\"KL div remove vs final: {kl_divs_remove_vs_final[i, j].mean():.2e} +- {kl_divs_remove_vs_final[i, j].std():.2e}\")\n",
    "        print(f\"KL div final vs remove: {kl_divs_final_vs_remove[i, j].mean():.2e} +- {kl_divs_final_vs_remove[i, j].std():.2e}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the kl results\n",
    "for i, model_size in enumerate(model_sizes):\n",
    "    for j, checkpoint_step in enumerate(checkpoints_steps):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axs[0].hist(kl_divs_remove_vs_retrained[i, j], bins=100, label=\"remove vs retrained\", alpha=0.5)\n",
    "        axs[0].hist(kl_divs_remove_vs_final[i, j], bins=100, label=\"remove vs final\", alpha=0.5)\n",
    "        axs[1].hist(kl_divs_retrained_vs_remove[i, j], bins=100, label=\"retrained vs remove\", alpha=0.5)\n",
    "        axs[1].hist(kl_divs_final_vs_remove[i, j], bins=100, label=\"final vs remove\", alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.title(f\"Model {model_size} step {checkpoint_step}\")\n",
    "        # label the axes\n",
    "        axs[0].set_xlabel(\"KL divergence\")\n",
    "        axs[0].set_ylabel(\"Count\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
